---
title: "A/B Testing"
layout: page
mathjax: true
---

The split or 'A/B' test is probably the most common statistical tool used in digital product management. We have a button on our website that delivers conversion to a certain goal. Do we get a higher conversion rate with a red button, or a blue button? We show each variant to half our users, measure the respective conversion rates and see which one is better. The important thing here is to ensure that the test is a fair one and that the results are meaningful and not simply due to chance. 

There are a variety of methods for achieving this, and a number of tools like [Optimizely](https://www.optimizely.com) are available which can manage the whole process. However they're also easy to misuse, and it's desirable to understand how they work.

Before proceeding further, it would be remiss not to point readers in the direction of Evan Miller's invaluable suite of [A/B testing tools](https://www.evanmiller.org/ab-testing/) and in-depth [explanatory articles](https://www.evanmiller.org/), which I have used on countless occasions. 

## Sample sizes and duration

All tests of this kind involve sampling, as by definition each measurement is based on a randomly chosen subset of a larger population. It's important to be sure that the sample is large enough - you wouldn't expect to determine if a coin was weighted by tossing it only three or four times.  


## Hypothesis testing and significance

The traditional and most familiar method for this statistical experiment is based on reasoning about the frequency with which you would expect to see a particular result depending on whether the hypothesis that version B is better than version A is true or false. 

## Comparing using confidence intervals

## Bayesian methods